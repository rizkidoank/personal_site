<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="Content-Language" content="en">

    <meta name="author" content="Rizki Rizki">
    <meta name="description" content="Pengantar Pada tulisan ini akan melanjutkan proses selanjutnya setelah mendapatkan data dengan Twitter API. Jika ingin mengunduh dataset tanpa mengambil online dari twitter, silakan unduh melalui link berikut :
 Tweet @RadioElshinta Stopwords Indonesia  Text Cleaning Setelah akuisisi data, langkah selanjutnya adalah Text Cleaning . Tahapan ini meliputi sub-proses antara lain stopwords removal, whitespaces stripping, dan stemming.
library(tm) library(SnowballC) load(file = &quot;elshinta.RData&quot;) tweets.df &lt;- twListToDF(tweets_data) corpus &lt;- Corpus(VectorSource(tweets.df$text)) # lowercase konten corpus &lt;- tm_map(corpus,content_transformer(tolower)) # hapus url, dan tanda baca removeURL &lt;- function(x) gsub(&quot;http[^[:space:]]*&quot;, &quot;&quot;, x) corpus &lt;- tm_map(corpus, content_transformer(removeURL)) corpus &lt;- tm_map(corpus, removePunctuation) # buat stopwords Indonesia file_stop &lt;- file(&quot;stopwords.">
    <meta name="keywords" content="blog,developer,personal,devops">

    

    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Twitter Mining with R : Tweet Analysis, Bagian 2"/>
<meta name="twitter:description" content="Pengantar Pada tulisan ini akan melanjutkan proses selanjutnya setelah mendapatkan data dengan Twitter API. Jika ingin mengunduh dataset tanpa mengambil online dari twitter, silakan unduh melalui link berikut :
 Tweet @RadioElshinta Stopwords Indonesia  Text Cleaning Setelah akuisisi data, langkah selanjutnya adalah Text Cleaning . Tahapan ini meliputi sub-proses antara lain stopwords removal, whitespaces stripping, dan stemming.
library(tm) library(SnowballC) load(file = &quot;elshinta.RData&quot;) tweets.df &lt;- twListToDF(tweets_data) corpus &lt;- Corpus(VectorSource(tweets.df$text)) # lowercase konten corpus &lt;- tm_map(corpus,content_transformer(tolower)) # hapus url, dan tanda baca removeURL &lt;- function(x) gsub(&quot;http[^[:space:]]*&quot;, &quot;&quot;, x) corpus &lt;- tm_map(corpus, content_transformer(removeURL)) corpus &lt;- tm_map(corpus, removePunctuation) # buat stopwords Indonesia file_stop &lt;- file(&quot;stopwords."/>

    <meta property="og:title" content="Twitter Mining with R : Tweet Analysis, Bagian 2" />
<meta property="og:description" content="Pengantar Pada tulisan ini akan melanjutkan proses selanjutnya setelah mendapatkan data dengan Twitter API. Jika ingin mengunduh dataset tanpa mengambil online dari twitter, silakan unduh melalui link berikut :
 Tweet @RadioElshinta Stopwords Indonesia  Text Cleaning Setelah akuisisi data, langkah selanjutnya adalah Text Cleaning . Tahapan ini meliputi sub-proses antara lain stopwords removal, whitespaces stripping, dan stemming.
library(tm) library(SnowballC) load(file = &quot;elshinta.RData&quot;) tweets.df &lt;- twListToDF(tweets_data) corpus &lt;- Corpus(VectorSource(tweets.df$text)) # lowercase konten corpus &lt;- tm_map(corpus,content_transformer(tolower)) # hapus url, dan tanda baca removeURL &lt;- function(x) gsub(&quot;http[^[:space:]]*&quot;, &quot;&quot;, x) corpus &lt;- tm_map(corpus, content_transformer(removeURL)) corpus &lt;- tm_map(corpus, removePunctuation) # buat stopwords Indonesia file_stop &lt;- file(&quot;stopwords." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://rizkidoank.com/2016/06/12/twitter-mining-with-r-tweet-analysis-bagian-2/" />
<meta property="article:published_time" content="2016-06-12T16:54:57+00:00" />
<meta property="article:modified_time" content="2016-06-12T16:54:57+00:00" />


    
      <base href="https://rizkidoank.com/2016/06/12/twitter-mining-with-r-tweet-analysis-bagian-2/">
    
    <title>
  Twitter Mining with R : Tweet Analysis, Bagian 2 · rizkidoank&#39;s Blog
</title>

    
      <link rel="canonical" href="https://rizkidoank.com/2016/06/12/twitter-mining-with-r-tweet-analysis-bagian-2/">
    

    <link href="https://fonts.googleapis.com/css?family=Lato:400,700%7CMerriweather:300,700%7CSource+Code+Pro:400,700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.13.0/css/all.css" integrity="sha384-Bfad6CLCknfcloXFOyFnlgtENryhrpZCe29RTifKEixXQZ38WheV+i/6YWSzkz3V" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css" integrity="sha256-l85OmPOjvil/SOvVt3HnSSjzF1TUMyT9eV0c2BzEGzU=" crossorigin="anonymous" />

    
      
      
      <link rel="stylesheet" href="/css/coder.min.3219ef62ae52679b7a9c19043171c3cd9f523628c2a65f3ef247ee18836bc90b.css" integrity="sha256-MhnvYq5SZ5t6nBkEMXHDzZ9SNijCpl8&#43;8kfuGINryQs=" crossorigin="anonymous" media="screen" />
    

    

    

    

    

    <link rel="icon" type="image/png" href="https://rizkidoank.com/images/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="https://rizkidoank.com/images/favicon-16x16.png" sizes="16x16">

    <meta name="generator" content="Hugo 0.72.0" />
  </head>

  
  
  <body class="colorscheme-light"
        onload=""
  >
    <main class="wrapper">
      <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="/">
      rizkidoank&#39;s Blog
    </a>
    
    <input type="checkbox" id="menu-toggle" />
    <label class="menu-button float-right" for="menu-toggle"><i class="fas fa-bars"></i></label>
    <ul class="navigation-list">
      
        
          <li class="navigation-item">
            <a class="navigation-link" href="https://rizkidoank.com/posts/">Blog</a>
          </li>
        
          <li class="navigation-item">
            <a class="navigation-link" href="https://rizkidoank.com/about/">About</a>
          </li>
        
      
      
    </ul>
    
  </section>
</nav>


      <div class="content">
        
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">Twitter Mining with R : Tweet Analysis, Bagian 2</h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fas fa-calendar"></i>
              <time datetime='2016-06-12T16:54:57Z'>
                June 12, 2016
              </time>
            </span>
            <span class="reading-time">
              <i class="fas fa-clock"></i>
              1-minute read
            </span>
          </div>
          
          <div class="tags">
  <i class="fas fa-tag"></i>
    <a href="/tags/r/">r</a>
      <span class="separator">•</span>
    <a href="/tags/programming/">programming</a>
      <span class="separator">•</span>
    <a href="/tags/text-mining/">text-mining</a></div>

        </div>
      </header>

      <div>
        
        <h2 id="pengantar">Pengantar</h2>
<p>Pada tulisan ini akan melanjutkan proses selanjutnya setelah mendapatkan data dengan Twitter API. Jika ingin mengunduh dataset tanpa mengambil online dari twitter, silakan unduh melalui link berikut :</p>
<ul>
<li><a href="https://rizkidoank.com/static/elshinta.RData">Tweet @RadioElshinta</a></li>
<li><a href="https://rizkidoank.com/static/stopwords.txt">Stopwords Indonesia</a></li>
</ul>
<h2 id="text-cleaning">Text Cleaning</h2>
<p>Setelah akuisisi data, langkah selanjutnya adalah <em>Text Cleaning</em> . Tahapan ini meliputi sub-proses antara lain stopwords removal, whitespaces stripping, dan stemming.</p>
<pre><code>library(tm)
library(SnowballC)
load(file = &quot;elshinta.RData&quot;)
tweets.df &lt;- twListToDF(tweets_data)

corpus &lt;- Corpus(VectorSource(tweets.df$text))

# lowercase konten
corpus &lt;- tm_map(corpus,content_transformer(tolower))

# hapus url, dan tanda baca
removeURL &lt;- function(x) gsub(&quot;http[^[:space:]]*&quot;, &quot;&quot;, x)
corpus &lt;- tm_map(corpus, content_transformer(removeURL))
corpus &lt;- tm_map(corpus, removePunctuation)

# buat stopwords Indonesia
file_stop &lt;- file(&quot;stopwords.txt&quot;,open = &quot;r&quot;)
id_stopwords &lt;- readLines(file_stop)
close(file_stop)
id_stopwords = c(id_stopwords, &quot;amp&quot;)

# hapus stopwords, angka, whitespace
corpus &lt;- tm_map(corpus, removeWords, id_stopwords)
corpus &lt;- tm_map(corpus, removeNumbers)
corpus &lt;- tm_map(corpus, stripWhitespace)
corpus &lt;- tm_map(corpus, PlainTextDocument)

# tampilkan konten tweet ke 125
writeLines(strwrap(corpus[[125]]$content))

# TDF dan DTF untuk corpus dataset elshinta
dtm = DocumentTermMatrix(corpus)
tdm = TermDocumentMatrix(corpus)
</code></pre>
<p>Untuk kasus ini, bahasa yang digunakan adalah bahasa Indonesia. Sedangkan pada R tidak tersedia untuk bahasa Indonesia. Sehingga, perlu membuat sendiri stopwords custom.</p>
<p><img src="https://rizkidoank.sgp1.digitaloceanspaces.com/rizkidoank/images/2016/06/twitter_mining_part_02_01.jpg" alt="Stopwords Bahasa Indonesia"></p>
<p><img src="https://rizkidoank.sgp1.digitaloceanspaces.com/rizkidoank/images/2016/06/twitter_mining_part_02_02.jpg" alt="Dataframe Tweets @RadioElshinta"></p>
<p><img src="https://rizkidoank.sgp1.digitaloceanspaces.com/rizkidoank/images/2016/06/twitter_mining_part_02_03.jpg" alt="Konten Tweet ke 120 dan 125"></p>
<p><img src="https://rizkidoank.sgp1.digitaloceanspaces.com/rizkidoank/images/2016/06/twitter_mining_part_02_04.jpg" alt="DTF dan TDF dari corpus"></p>
<p>Setelah dilakukan Text Cleaning, maka data siap diolah menjadi informasi lain. Tahap selanjutnya yaitu membuat statistik term frequency, dan membuat wordcloud dari TDF.</p>

      </div>


      <footer>
        


        <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "rizkidoank" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
        
        
      </footer>
    </article>

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script type="text/javascript" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js" id="MathJax-script"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [
          ['$', '$'], ['\\(', '\\)']
        ],
        processEscapes: true,
        processEnvironments: true
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
  </script>
  </section>

      </div>

      
  <footer class="footer">
    <section class="container">
      
      
        ©
        
        2020
         Rizki Rizki 
      
      
         · 
        Powered by <a href="https://gohugo.io/">Hugo</a> & <a href="https://github.com/luizdepra/hugo-coder/">Coder</a>.
      
      
        
      
    </section>
  </footer>

    </main>

    

    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-78100671-2', 'auto');
	
	ga('send', 'pageview');
}
</script>


    

  </body>

</html>
